{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81282902-0401-4319-a7f6-ba3b21fb893d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install jsonformer\n",
    "%pip install relik\n",
    "%pip install gliner\n",
    "%pip install sentencepiece\n",
    "%pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7fae0294-8513-438b-a145-bf33acf32901",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import  RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import os\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from neo4j import GraphDatabase\n",
    "# from yfiles_jupyter_graphs import GraphWidget\n",
    "from langchain_community.vectorstores import Neo4jVector\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores.neo4j_vector import remove_lucene_chars\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from transformers import AutoTokenizer, pipeline, AutoModelForCausalLM, BitsAndBytesConfig, AutoModel\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from llm import LLMGraphTransformer\n",
    "\n",
    "import torch\n",
    "from typing import Optional, List\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    PromptTemplate,\n",
    ")\n",
    "import json_repair\n",
    "from langchain_community.graphs.graph_document import GraphDocument, Node, Relationship\n",
    "from langchain_core.documents import Document\n",
    "from jsonformer import Jsonformer\n",
    "from relik.inference.data.objects import RelikOutput\n",
    "from gliner import GLiNER\n",
    "from langchain_huggingface import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d3f61d6-b385-4cb9-b5e4-191323227720",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDocuments(path):\n",
    "    if not os.path.exists(path):\n",
    "        print(\"Path do not exist\")\n",
    "        return []\n",
    "    documents = []\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            content = \"\"\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    content = f.read()\n",
    "            except Exception as e:\n",
    "                print(f\"Errore nella lettura di {file_path}: {e}\")\n",
    "\n",
    "            documents.append(Document(\n",
    "                page_content=content,\n",
    "                metadata={\"path\": file_path,\n",
    "                        \"title\": content.split(\"\\n\")[2],\n",
    "                        \"file_name\": file}))\n",
    "    return documents\n",
    "\n",
    "def cleanMarkup(llm, text):\n",
    "    template = \"\"\"\n",
    "    You are given a markup text. Your task is to remove any unnecessary or non-informative parts, such as:\n",
    "    - Tags, unless they contain useful content.\n",
    "    - Repeated phrases or sections.\n",
    "    - Decorative characters or symbols.\n",
    "    - Empty lines or spaces.\n",
    "\n",
    "    Please leave informative links.\n",
    "\n",
    "    Output only the cleaned text, without any additional explanation or markup.\n",
    "\n",
    "    {text}\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "    chain = prompt | llm\n",
    "\n",
    "    return chain.invoke({\"text\": text}) \n",
    "\n",
    "def cleanDocuments(llm, documents, create_files=False):\n",
    "    if create_files:\n",
    "        output_directory = \"cleaned_documents\"\n",
    "        os.makedirs(output_directory, exist_ok=True)\n",
    "    i = 0\n",
    "    for d in documents:\n",
    "        i+=1\n",
    "        print(f\"Cleaning the document number: {i}/{len(documents)}\")\n",
    "        d.page_content = cleanMarkup(llm, d.page_content)\n",
    "        if create_files:\n",
    "            output_path = os.path.join(output_directory, d.metadata['file_name'])\n",
    "            with open(output_path, \"w\") as file:\n",
    "                file.write(d.page_content)\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "666eed36-6fe8-4225-8c82-4170741fa668",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Entities(BaseModel):\n",
    "    \"\"\"Identifying information about entities.\"\"\"\n",
    "\n",
    "    names: list[str] = Field(\n",
    "        ...,\n",
    "        description=\"All the person, organization, or business entities that \"\n",
    "        \"appear in the input text\",\n",
    "    )\n",
    "    \n",
    "class UnstructuredRelation(BaseModel):\n",
    "    head: str = Field(\n",
    "        description=(\n",
    "            \"extracted head entity like Microsoft, Apple, John. \"\n",
    "            \"Must use human-readable unique identifier.\"\n",
    "        )\n",
    "    )\n",
    "    head_type: str = Field(\n",
    "        description=\"type of the extracted head entity like Person, Company, etc\"\n",
    "    )\n",
    "    relation: str = Field(description=\"relation between the head and the tail entities\")\n",
    "    tail: str = Field(\n",
    "        description=(\n",
    "            \"extracted tail entity like Microsoft, Apple, John. \"\n",
    "            \"Must use human-readable unique identifier.\"\n",
    "        )\n",
    "    )\n",
    "    tail_type: str = Field(\n",
    "        description=\"type of the extracted tail entity like Person, Company, etc\"\n",
    "    )\n",
    "\n",
    "def getHuggingFaceModel(model_id, hf_token):\n",
    "    # model_id = \"microsoft/Phi-3-mini-128k-instruct\"\n",
    "    # model_id = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "    #model_id = \"microsoft/Phi-3.5-mini-instruct\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, token = hf_token)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        quantization_config=BitsAndBytesConfig(load_in_4bit=True),\n",
    "        device_map=\"cuda\",\n",
    "        trust_remote_code = True, #Added for Phi-3-mini-128k\n",
    "        token = hf_token\n",
    "        #attn_implementation=\"flash_attention_2\", # if you have an ampere GPU (RTX3090 OK, T4(Colab) NON OK)\n",
    "    )\n",
    "    pipe = pipeline(\"text-generation\",\n",
    "                    model=model,\n",
    "                    tokenizer=tokenizer,\n",
    "                    max_new_tokens=1024,\n",
    "                    top_k=50,\n",
    "                    temperature=0.1,\n",
    "                    return_full_text = False)\n",
    "    llm = HuggingFacePipeline(pipeline=pipe,\n",
    "                            pipeline_kwargs={\"return_full_text\": False}) # <----- IMPORTANT !!!\n",
    "    return tokenizer, model, pipe, llm\n",
    "\n",
    "def getHuggingFacePipe(model_id, hf_token):\n",
    "    # model_id = \"microsoft/Phi-3-mini-128k-instruct\"\n",
    "    # model_id = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "    #model_id = \"microsoft/Phi-3.5-mini-instruct\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, token = hf_token)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        quantization_config=BitsAndBytesConfig(load_in_4bit=True),\n",
    "        device_map=\"cuda\",\n",
    "        trust_remote_code = True, #Added for Phi-3-mini-128k\n",
    "        token = hf_token\n",
    "        #attn_implementation=\"flash_attention_2\", # if you have an ampere GPU (RTX3090 OK, T4(Colab) NON OK)\n",
    "    )\n",
    "    pipe = pipeline(\"text-generation\",\n",
    "                    model=model,\n",
    "                    tokenizer=tokenizer,\n",
    "                    max_new_tokens=1024,\n",
    "                    top_k=50,\n",
    "                    temperature=0.1,\n",
    "                    return_full_text = False)\n",
    "    return pipe\n",
    "\n",
    "def extractNodesAndRelationships(graph_document):\n",
    "    node_types = set()\n",
    "    relationships = set()\n",
    "    for d in graph_documents:\n",
    "        for n in d.nodes:\n",
    "            node_types.add(n.type)\n",
    "        for r in d.relationships:\n",
    "            relationships.add(r.type)\n",
    "    return node_types, relationships\n",
    "\n",
    "def getCUDAMemoryInfo():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    t = torch.cuda.get_device_properties(0).total_memory\n",
    "    r = torch.cuda.memory_reserved(0)\n",
    "    a = torch.cuda.memory_allocated(0)\n",
    "    f = r-a  # free inside reserved\n",
    "    print(t)\n",
    "    print(r)\n",
    "    print(a)\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aabf352a-29f0-4aa0-922f-8346c21f503f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_full_text_query(input: str) -> str:\n",
    "    words = [el for el in remove_lucene_chars(input).split() if el]\n",
    "    if not words:\n",
    "        return \"\"\n",
    "    full_text_query = \" AND \".join([f\"{word}~2\" for word in words])\n",
    "    print(f\"Generated Query: {full_text_query}\")\n",
    "    return full_text_query.strip()\n",
    "\n",
    "# Fulltext index query\n",
    "def graph_retriever(question: str, graph, gliner_model, entities) -> str:\n",
    "    \"\"\"\n",
    "    Collects the neighborhood of entities mentioned\n",
    "    in the question\n",
    "    \"\"\"\n",
    "    result = \"\"\n",
    "    # entities = entity_chain.invoke({\"question\": question})\n",
    "    # entities = [s.text for s in relik(question).spans]\n",
    "    entities = [entity[\"text\"] for entity in gliner_model.predict_entities(question, entities)]\n",
    "    print(entities)\n",
    "    for entity in entities:\n",
    "        attempts = 0\n",
    "        max_attempts = 5\n",
    "        while attempts < max_attempts:\n",
    "            try:\n",
    "                response = graph.query(\n",
    "                    \"\"\"CALL db.index.fulltext.queryNodes('keyword', $query, {limit:2})\n",
    "                    YIELD node,score\n",
    "                    CALL {\n",
    "                      WITH node\n",
    "                      MATCH (node)-[r:!MENTIONS]->(neighbor)\n",
    "                      RETURN node.id + ' - ' + type(r) + ' -> ' + neighbor.id AS output\n",
    "                      UNION ALL\n",
    "                      WITH node\n",
    "                      MATCH (node)<-[r:!MENTIONS]-(neighbor)\n",
    "                      RETURN neighbor.id + ' - ' + type(r) + ' -> ' +  node.id AS output\n",
    "                    }\n",
    "                    RETURN output LIMIT 50\n",
    "                    \"\"\",\n",
    "                    {\"query\": generate_full_text_query(entity)},\n",
    "                )\n",
    "                result += \"\\n\".join([el['output'] for el in response])\n",
    "            except Exception as e:\n",
    "                attempts += 1\n",
    "                print(f\"Attempt {attempts} failed due to {type(e).__name__}: {str(e)}\")\n",
    "                \n",
    "                if attempts == max_attempts:\n",
    "                    print(f\"Max attempts reached. Unable to run the query.\")\n",
    "                    raise\n",
    "    return result\n",
    "\n",
    "def graph_retriever_2(question: str, graph, gliner_model, entities) -> str:\n",
    "    \"\"\"\n",
    "    Collects the neighborhood of entities mentioned\n",
    "    in the question\n",
    "    \"\"\"\n",
    "    entities = [entity[\"text\"] for entity in gliner_model.predict_entities(question, entities)]\n",
    "    print(entities)\n",
    "    \n",
    "    result = \"\"\n",
    "    for entity in entities:\n",
    "        attempts = 0\n",
    "        max_attempts = 5\n",
    "        while attempts < max_attempts:\n",
    "            try:\n",
    "                response = graph.query(\n",
    "                    \"\"\"CALL db.index.fulltext.queryNodes('keyword', $query, {limit:2})\n",
    "                    YIELD node, score\n",
    "                    RETURN node, score\n",
    "                    \"\"\",\n",
    "                    {\"query\": generate_full_text_query(entity)},\n",
    "                )\n",
    "                \n",
    "                print(\"Nodes and Scores:\")\n",
    "                for record in response:\n",
    "                    node = record['node']\n",
    "                    score = record['score']\n",
    "                    node_id = node.get(\"id\")\n",
    "                    node_name = node.get(\"file_name\")\n",
    "                    node_title = node.get(\"title\")\n",
    "                    print(f\"Node ID: {node_id}, Title: {node_title}, Name: {node_name}, Score: {score}\")\n",
    "                \n",
    "                result += \"\\n\".join([el['output'] for el in response if 'output' in el])\n",
    "                break\n",
    "            except Exception as e:\n",
    "                attempts += 1\n",
    "                print(f\"Attempt {attempts} failed due to {type(e).__name__}: {str(e)}\")\n",
    "                \n",
    "                if attempts == max_attempts:\n",
    "                    print(f\"Max attempts reached. Unable to run the query.\")\n",
    "                    raise\n",
    "    return result\n",
    "\n",
    "def full_retriever(question: str):\n",
    "    graph_data = graph_retriever(question)\n",
    "    vector_data = [el.page_content for el in vector_retriever.invoke(question)]\n",
    "    final_data = f\"\"\"Graph data:\n",
    "    {graph_data}\n",
    "    vector data:\n",
    "    {\"#Document \". join(vector_data)}\n",
    "        \"\"\"\n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "raw",
   "id": "adebe0d3-5e2a-4b30-b8e9-e80f75e0c5a2",
   "metadata": {},
   "source": [
    "Loading Environment variables and downloading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca582de6-514d-4ed5-a32c-66ff8327096c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "de9d3e05-a241-4fae-9580-9cdb08d89ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-09-11 17:22:58,969] [INFO] [sentence_transformers.SentenceTransformer.__init__:113] [PID:19406] Load pretrained SentenceTransformer: nomic-ai/nomic-embed-text-v1.5\n",
      "[2024-09-11 17:23:02,250] [WARNING] [transformers_modules.nomic-ai.nomic-bert-2048.4bb68f63016e88e53e48df904c6ab4e6f718e198.modeling_hf_nomic_bert.from_pretrained:443] [PID:19406] <All keys matched successfully>\n",
      "[2024-09-11 17:23:02,703] [INFO] [sentence_transformers.SentenceTransformer.__init__:219] [PID:19406] Use pytorch device_name: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 4 files: 100%|██████████| 4/4 [00:00<00:00, 40524.68it/s]\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py:551: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#LLM Model\n",
    "tokenizer, model, pipe, llm = getHuggingFaceModel(model_id=\"meta-llama/Meta-Llama-3.1-8B-Instruct\", hf_token = os.getenv(\"HF_TOKEN\"))\n",
    "llm_transformer = LLMGraphTransformer(llm=llm)\n",
    "\n",
    "#Embedding Model\n",
    "model_name = \"nomic-ai/nomic-embed-text-v1.5\"\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs={\"trust_remote_code\": True}\n",
    ")\n",
    "\n",
    "#NER Model\n",
    "gliner_model = GLiNER.from_pretrained(\"urchade/gliner_multi\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "027ffd2e-09b4-4ba8-ac9d-5dc8dcca44ae",
   "metadata": {},
   "source": [
    "Reading and preparing the documents for the graph generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c386c921-e581-4856-826a-b9c0fa5f43d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loader = TextLoader(file_path=\"dummytext.txt\")\n",
    "# docs = loader.load()\n",
    "crawl_documents = getDocuments(\"crawl\")\n",
    "\n",
    "# cleaned_documents = cleanDocuments(llm, documents)\n",
    "cleaned_documents = getDocuments(\"cleaned_documents\") #Replace with row above\n",
    "\n",
    "#Documents splitting\n",
    "# chuck_size = 750\n",
    "# text_splitter = RecursiveCharacterTextSplitter(chunk_size=chuck_size, chunk_overlap=chuck_size*0.2)\n",
    "# documents = text_splitter.split_documents(documents=cleaned_documents)\n",
    "# print(f\"Went from {len(cleaned_documents)} documents to {len(documents)} splitted documents\")\n",
    "\n",
    "selected_file_names = [\"155.txt\", \"60.txt\", \"42.txt\", \"99.txt\", \"124.txt\", \"117.txt\", \"207.txt\", \"8.txt\", \"77.txt\"]\n",
    "selected_documents = [d for d in cleaned_documents if d.metadata[\"file_name\"] in selected_file_names]\n",
    "documents = selected_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f14c4a91-f1c6-4cb1-8502-2b4d032a9454",
   "metadata": {},
   "outputs": [],
   "source": [
    "JSON_object_schema = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"triplets\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\n",
    "                \"type\": \"object\",\n",
    "                \"minItems\": 5,\n",
    "                \"properties\": {\n",
    "                    \"head\": { \"type\": \"string\" },\n",
    "                    \"head_type\": { \"type\": \"string\" },\n",
    "                    \"relation\": { \"type\": \"string\" },\n",
    "                    \"tail\": { \"type\": \"string\" },\n",
    "                    \"tail_type\": { \"type\": \"string\" }\n",
    "                },\n",
    "                \"required\": [\"head\", \"head_type\", \"relation\", \"tail\", \"tail_type\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"required\": [\"triplets\"]\n",
    "}\n",
    "\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Extract at least 5 triplets from the following text in the format of head node, head node type, relation, tail node, and tail node type. \n",
    "Ensure the extracted triplets follow this JSON schema structure:\n",
    "\n",
    "1. Each triplet must include:\n",
    "   - head: the subject entity\n",
    "   - head_type: type/category of the head entity\n",
    "   - relation: the relationship between the head and tail\n",
    "   - tail: the object entity\n",
    "   - tail_type: type/category of the tail entity\n",
    "\n",
    "2. The output must strictly follow this format in the 'triplets' array, without any additional information.\n",
    "\n",
    "Now, generate the 5 or moreknowledge graph triplets based on the provided text, formatted according to this schema:\n",
    "\n",
    "{documents[1].page_content}\n",
    "\"\"\"\n",
    "\n",
    "jsonformer = Jsonformer(model, tokenizer, JSON_object_schema, prompt)\n",
    "generated_data = jsonformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f616f3d3-3a71-4d56-a8d4-80c78aa92d56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'head': 'Degree Programme',\n",
       "  'head_type': 'Academic Programme',\n",
       "  'relation': 'sets',\n",
       "  'tail': 'education objectives',\n",
       "  'tail_type': 'Learning Outcomes'}]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_data.get(\"triplets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "864fd45d-a2a1-467c-bedb-caf39b2dc0de",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/bitsandbytes/nn/modules.py:435: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "155.txt\n",
      "42.txt\n",
      "99.txt\n",
      "124.txt\n",
      "117.txt\n",
      "60.txt\n",
      "207.txt\n",
      "8.txt\n",
      "77.txt\n"
     ]
    }
   ],
   "source": [
    "graph_documents = llm_transformer.convert_to_graph_documents(documents[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "19dd35b0-0603-47ca-acfb-cfad02740f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# #Removing invalid nodes\n",
    "for d in graph_documents:\n",
    "    print(len(d.nodes))\n",
    "    for n in d.nodes:\n",
    "        print(n)\n",
    "        # if n.id == \"\" or n.id is None or n.type == \"\" or n.type is None:\n",
    "        #     print(n)\n",
    "        #     d.nodes.remove(n)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f9e9f2c7-9f1c-4f48-959f-d13bfb91693a",
   "metadata": {},
   "source": [
    "Connect to graph db and add the generated entities and relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1cfea129-06c4-442c-b065-6b5f187e211e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "graph = Neo4jGraph(url= \"neo4j+s://bbef2ff2.databases.neo4j.io\", username=\"neo4j\", password=\"fdZslu0qGuZhCiR9pasipKRR-iLDgz9AMp8KVS9Uf2s\")\n",
    "graph.add_graph_documents(\n",
    "    graph_documents,\n",
    "    baseEntityLabel=True,\n",
    "    include_source=True\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d460c2d3-9d6f-45c9-9b24-71c4461229db",
   "metadata": {},
   "source": [
    "Create embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11906f2d-a205-4659-b89c-604534f43307",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a1fc38-6114-456e-a205-17ca1cbbec9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_index = Neo4jVector.from_existing_graph(\n",
    "    embeddings,\n",
    "    search_type=\"hybrid\",\n",
    "    node_label=\"Document\",\n",
    "    text_node_properties=[\"text\"],\n",
    "    embedding_node_property=\"embedding\"\n",
    ")\n",
    "vector_retriever = vector_index.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b131d29e-4eae-430b-a157-e08a0e0d640a",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_types, relationships = extractNodesAndRelationships(graph_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bf88fc-55a5-4cf5-9c67-c2e2172562c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552fe02a-3b20-4ff2-b05b-5d108cd8a233",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = {\n",
    "    1: \"Who is the head of the Computer Science department at Stanford University?\",\n",
    "    2: \"What do you know about Elon Musk and SpaceX?\",\n",
    "    3: \"Could you tell me who is the CEO of Tesla and what the name of the founder of SpaceX is?\",\n",
    "    4: \"I am looking for information about the current President of the United States and the Prime Minister of the United Kingdom.\",\n",
    "    5: \"Who wrote the book 'To Kill a Mockingbird' and what is the name of the author of '1984'?\",\n",
    "    6: \"Can you provide the names of the actors who starred in the film 'Inception' and who directed it?\",\n",
    "    7: \"I want to know who the Chancellor of Germany is and what the name of the leader of the French Republic is.\",\n",
    "    8: \"Please give me the names of the people who won the Nobel Prize in Literature in 2023 and 2022.\",\n",
    "    9: \"What are the names of the characters portrayed by Keanu Reeves in the Matrix series and who is the director of the film?\",\n",
    "    10: \"Can you identify the name of the founder of Microsoft and the person who is the current Secretary-General of the United Nations?\",\n",
    "    11: \"Tell me who the lead vocalist of the band Coldplay is and the name of the band that performed 'Bohemian Rhapsody'.\",\n",
    "    12: \"Who are the authors of the books 'The Catcher in the Rye' and 'Pride and Prejudice'?\",\n",
    "    13: \"Where is the Università di Bologna?\"\n",
    "}\n",
    "\n",
    "student_questions = {\n",
    "    1: \"Where can I find the academic calendar and important deadlines?\",\n",
    "    2: \"How do I register for classes, and what is the process for adding or dropping courses?\",\n",
    "    3: \"What are the requirements for my major, and where can I find my degree plan?\",\n",
    "    4: \"Where is the library, and how do I access online resources or research databases?\",\n",
    "    5: \"How can I contact my academic advisor, and when should I meet with them?\",\n",
    "    6: \"What is the best way to get involved in student organizations or extracurricular activities?\",\n",
    "    7: \"Where can I find information about on-campus housing and meal plans?\",\n",
    "    8: \"How do I get my student ID card, and what is it used for?\",\n",
    "    9: \"What health and wellness services are available on campus?\",\n",
    "    10: \"Where do I go if I need help with my coursework or tutoring services?\",\n",
    "    11: \"How do I apply for financial aid, and where can I check my student account or pay tuition?\",\n",
    "    12: \"What are the university’s policies on academic integrity and plagiarism?\",\n",
    "    13: \"Where is the career center, and how can it help me with internships or job placements?\",\n",
    "    14: \"What campus safety resources are available, and how can I contact campus security?\",\n",
    "    15: \"How do I access the university’s transportation system or find parking on campus?\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5b7332da-d0ff-45c7-bb9d-140172e55ae3",
   "metadata": {},
   "source": [
    "Initialize relik model for text extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f283c6a6-d303-4034-8b0c-b289adfd7c12",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb76a10-8837-4ccc-b808-784ded51f100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for q in student_questions.values():\n",
    "#     entities = model.predict_entities(q, labels)\n",
    "    \n",
    "#     print(\"\\n\" + q)\n",
    "#     for entity in entities:\n",
    "#         print(entity[\"text\"], \"=>\", entity[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4833e6-0b67-425e-9ed1-480f10658f81",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Output: \" + graph_retriever_2(\"Does Unibo has a Youtube account?\", graph, gliner_model, node_types))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
